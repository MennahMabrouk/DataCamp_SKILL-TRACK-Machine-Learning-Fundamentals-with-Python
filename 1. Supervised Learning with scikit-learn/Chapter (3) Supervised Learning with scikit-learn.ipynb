{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning with scikit-learn\n",
    "\n",
    "## Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter Slides\n",
    "\n",
    "https://drive.google.com/drive/folders/1YH14o1I9KlcEpPEvtuDh89hxY68C4ibe?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy \n",
    "\n",
    "Common metric used to evaluate the performance of a classification model. However, it can be misleading in some cases, **such as imbalanced datasets**, where the majority class dominates the minority class, leading to high accuracy even with poor performance on the minority class.\n",
    "\n",
    "In such cases, metrics such as **precision**, **recall**, **F1-score**, **AUC-ROC**, and confusion matrix can provide a more comprehensive evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a class imbalanced scenario, accuracy can be misleading and can provide a false sense of the model's performance. A model that always predicts the majority class can achieve high accuracy but fail to identify the minority class, which is the important class in this case (fraudulent transactions). \n",
    "\n",
    "To address class imbalance, you can use metrics such as **precision**, **recall**,**F1-score**, **AUC-ROC**, and the confusion matrix to get a better understanding of the model's performance. \n",
    "\n",
    "Precision measures the proportion of true positive predictions out of all positive predictions, recall measures the proportion of true positive predictions out of all actual positive cases, and F1-score is the harmonic mean of precision and recall. \n",
    "\n",
    "**AUC-ROC (Area Under the Receiver Operating Characteristic Curve)** measures the model's ability to distinguish between positive and negative classes. The confusion matrix provides a detailed summary of the true positive, true negative, false positive, and false negative predictions made by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to assess the performance of a classification model, depending on the problem you are trying to solve and the distribution of your data. Some commonly used metrics include:\n",
    "\n",
    "<p>\n",
    "  <ol>\n",
    "    <li><b>Accuracy:</b><br>\n",
    "      The proportion of correct predictions made by the model, calculated as (True Positives + True Negatives) / Total.</li>\n",
    "    <br>\n",
    "    <li><b>Precision:</b><br>\n",
    "      The proportion of true positive predictions out of all positive predictions, calculated as True Positives / (True Positives + False Positives).</li>\n",
    "    <br>\n",
    "    <li><b>Recall (or Sensitivity or True Positive Rate):</b><br>\n",
    "      The proportion of true positive predictions out of all actual positive cases, calculated as True Positives / (True Positives + False Negatives).</li>\n",
    "    <br>\n",
    "    <li><b>F1-Score:</b><br>\n",
    "      The harmonic mean of precision and recall, calculated as 2 * (Precision * Recall) / (Precision + Recall).</li>\n",
    "    <br>\n",
    "    <li><b>AUC-ROC (Area Under the Receiver Operating Characteristic Curve):</b><br>\n",
    "      A measure of the model's ability to distinguish between positive and negative classes, represented by a graph that plots the True Positive Rate against the False Positive Rate.</li>\n",
    "    <br>\n",
    "    <li><b>Confusion Matrix:</b><br>\n",
    "      A table that summarizes the true positive, true negative, false positive, and false negative predictions made by the model.</li>\n",
    "  </ol>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "Metric used in classification to evaluate the quality of positive predictions made by a model. It measures the proportion of true positive predictions out of all positive predictions made by the model. Precision is calculated as:\n",
    "\n",
    "<p style=\"background-color:LavenderBlush;\"><b>Precision = True Positives / (True Positives + False Positives)</b></p>\n",
    "\n",
    "Where True Positives (TP) are the number of samples that are correctly predicted as positive by the model, and False Positives (FP) are the number of samples that are incorrectly predicted as positive by the model. Precision provides information on the ability of the model to make correct positive predictions. High precision means that the model is making fewer false positive predictions, and thus has a low rate of false alarms.\n",
    "\n",
    "Precision should be used in combination with other metrics, such as recall and F1-score, to get a more complete understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "\n",
    "Known as Sensitivity or True Positive Rate, is a metric used in classification to evaluate the ability of a model to identify positive instances. It measures the proportion of true positive predictions made by the model out of all actual positive instances. Recall is calculated as:\n",
    "\n",
    "<p style=\"background-color:LavenderBlush;\"><b>Recall = True Positives / (True Positives + False Negatives)\n",
    "</b></p>\n",
    "\n",
    "Where True Positives (TP) are the number of samples that are correctly predicted as positive by the model, and False Negatives (FN) are the number of samples that are incorrectly predicted as negative by the model. Recall provides information on the ability of the model to find all positive instances. High recall means that the model is able to identify most of the positive instances.\n",
    "\n",
    "Recall should be used in combination with other metrics, such as precision and F1-score, to get a more complete understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The F1 score\n",
    "\n",
    "A measure of a model's accuracy that balances precision and recall. It is the harmonic mean of precision and recall\n",
    "\n",
    "<p style=\"background-color:Lavender;\"><b>calculated as 2 * (Precision * Recall) / (Precision + Recall)\n",
    "</b></p>\n",
    "\n",
    "The F1 score is a useful metric when there is an imbalance between precision and recall, or when the cost of false positives and false negatives is not equal. A high F1 score indicates a good balance between precision and recall, whereas a low F1 score indicates a poor balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding on a primary metric\n",
    "\n",
    "As you have seen, several metrics can be useful to evaluate the performance of classification models, including accuracy, precision, recall, and F1-score.\n",
    "\n",
    "In this exercise, you will be provided with three different classification problems, and your task is to select the problem where precision is best suited as the primary metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A model predicting the presence of cancer as the positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing a diabetes prediction classifier\n",
    "\n",
    "In this chapter you'll work with the diabetes_df dataset introduced previously.\n",
    "\n",
    "The goal is to predict whether or not each individual is likely to have diabetes based on the features body mass index (BMI) and age (in years). Therefore, it is a binary classification problem. A target value of 0 indicates that the individual does not have diabetes, while a value of 1 indicates that the individual does have diabetes.\n",
    "\n",
    "diabetes_df has been preloaded for you as a pandas DataFrame and split into X_train, X_test, y_train, and y_test. In addition, a KNeighborsClassifier() has been instantiated and assigned to knn.\n",
    "\n",
    "You will fit the model, make predictions on the test set, then produce a confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>glucose</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>triceps</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>dpf</th>\n",
       "      <th>age</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnancies  glucose  diastolic  triceps  insulin   bmi    dpf  age  \\\n",
       "0            6      148         72       35        0  33.6  0.627   50   \n",
       "1            1       85         66       29        0  26.6  0.351   31   \n",
       "2            8      183         64        0        0  23.3  0.672   32   \n",
       "3            1       89         66       23       94  28.1  0.167   21   \n",
       "4            0      137         40       35      168  43.1  2.288   33   \n",
       "\n",
       "   diabetes  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('/home/minmin/Downloads/diabetes_clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "Import confusion_matrix and classification_report.\n",
    "\n",
    "Fit the model to the training data.\n",
    "\n",
    "Predict the labels of the test set, storing the results as y_pred.\n",
    "\n",
    "Compute and print the confusion matrix and classification report for the test labels versus the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.57      0.28         7\n",
      "          24       0.00      1.00      0.00         0\n",
      "          40       0.00      1.00      0.00         0\n",
      "          44       0.00      0.00      0.00         2\n",
      "          46       0.00      1.00      0.00         0\n",
      "          48       0.00      0.00      0.00         1\n",
      "          50       0.00      0.00      0.00         5\n",
      "          52       0.00      0.00      0.00         1\n",
      "          54       0.00      0.00      0.00         4\n",
      "          55       0.00      1.00      0.00         0\n",
      "          56       0.00      0.00      0.00         2\n",
      "          58       0.11      0.67      0.19         3\n",
      "          60       0.12      0.11      0.12         9\n",
      "          62       0.00      0.00      0.00         8\n",
      "          64       0.00      0.00      0.00         9\n",
      "          65       1.00      0.00      0.00         4\n",
      "          66       0.10      0.20      0.13         5\n",
      "          68       0.00      0.00      0.00         8\n",
      "          70       0.00      0.00      0.00         7\n",
      "          72       0.00      0.00      0.00         7\n",
      "          74       0.00      0.00      0.00         7\n",
      "          75       1.00      0.00      0.00         1\n",
      "          76       0.00      0.00      0.00         5\n",
      "          78       0.00      0.00      0.00         9\n",
      "          80       0.00      0.00      0.00        11\n",
      "          82       0.00      0.00      0.00         7\n",
      "          84       0.00      0.00      0.00         9\n",
      "          85       1.00      0.00      0.00         1\n",
      "          86       0.00      0.00      0.00         4\n",
      "          88       1.00      0.00      0.00         8\n",
      "          90       1.00      0.00      0.00         3\n",
      "          92       1.00      0.00      0.00         2\n",
      "          94       1.00      0.00      0.00         1\n",
      "          96       0.00      1.00      0.00         0\n",
      "          98       1.00      0.00      0.00         1\n",
      "         100       1.00      0.00      0.00         1\n",
      "         106       1.00      0.00      0.00         1\n",
      "         108       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.05       154\n",
      "   macro avg       0.30      0.17      0.02       154\n",
      "weighted avg       0.18      0.05      0.03       154\n",
      "\n",
      "[[4 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data into a Pandas dataframe\n",
    "df = pd.read_csv('/home/minmin/Downloads/diabetes_clean.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df.drop('diastolic', axis=1)\n",
    "y = df['diastolic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the KNN classifier on the training data\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Logistic regression\n",
    "\n",
    "Type of regression analysis used for solving binary classification problems, i.e. problems where the dependent variable can only take two values, 0 or 1. The output of logistic regression is a probability that a given data point belongs to one of the two classes. The threshold value of 0.5 is used to make the final class prediction, where if the probability is greater than 0.5, it is labeled as 1, otherwise it is labeled as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a logistic regression model\n",
    "\n",
    "In this exercise, you will build a logistic regression model using all features in the diabetes_df dataset. The model will be used to predict the probability of individuals in the test set having a diabetes diagnosis.\n",
    "\n",
    "The diabetes_df dataset has been split into X_train, X_test, y_train, and y_test, and preloaded for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "Import LogisticRegression.\n",
    "\n",
    "Instantiate a logistic regression model, logreg.\n",
    "\n",
    "Fit the model to the training data.\n",
    "\n",
    "Predict the probabilities of each individual in the test set having a diabetes diagnosis, storing the array of positive probabilities as y_pred_probs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00222979 0.00123342 0.00083997 0.00235243 0.00226814 0.00173799\n",
      " 0.000134   0.00022037 0.00161866 0.00208011]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minmin/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/minmin/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_probs = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(y_pred_probs[:10])\n",
    "\n",
    "\"\"\"\n",
    "# Import libraries\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate the model\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "\n",
    "# Use RFE to select the most relevant features\n",
    "rfe = RFE(logreg, n_features_to_select=10)\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "# Fit the model on the selected features\n",
    "logreg.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_probs = logreg.predict_proba(rfe.transform(X_test))[:, 1]\n",
    "\n",
    "print(y_pred_probs[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC curve\n",
    "\n",
    "Now you have built a logistic regression model for predicting diabetes status, you can plot the ROC curve to visualize how the true positive rate and false positive rate vary as the decision threshold changes.\n",
    "\n",
    "The test labels, y_test, and the predicted probabilities of the test features belonging to the positive class, y_pred_probs, have been preloaded for you, along with matplotlib.pyplot as plt.\n",
    "\n",
    "You will create a ROC curve and then interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions 1/2\n",
    "\n",
    "Import roc_curve.\n",
    "\n",
    "Calculate the ROC curve values, using y_test and y_pred_probs, and unpacking the results into fpr, tpr, and thresholds.\n",
    "\n",
    "Plot true positive rate against false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Import roc_curve\\nfrom sklearn.metrics import roc_curve\\n\\n# Generate ROC curve values: fpr, tpr, thresholds\\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\\n\\nplt.plot([0, 1], [0, 1], 'k--')\\n\\n# Plot tpr against fpr\\nplt.plot(fpr, tpr)\\nplt.xlabel('False Positive Rate')\\nplt.ylabel('True Positive Rate')\\nplt.title('ROC Curve for Diabetes Prediction')\\nplt.show()\\n# Import roc_curve\\nfrom sklearn.metrics import roc_curve\\n\\n# Generate ROC curve values: fpr, tpr, thresholds\\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\\n\\nplt.plot([0, 1], [0, 1], 'k--')\\n\\n# Plot tpr against fpr\\nplt.plot(fpr, tpr)\\nplt.xlabel('False Positive Rate')\\nplt.ylabel('True Positive Rate')\\nplt.title('ROC Curve for Diabetes Prediction')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code to try \n",
    "\n",
    "\"\"\"\n",
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Instantiate the encoder\n",
    "encoder = OneHotEncoder()\n",
    "y_test_bin = pd.get_dummies(y_test).to_numpy()\n",
    "\n",
    "# Fit and transform the 1-dimensional y_test array to a 2-dimensional binary array\n",
    "y_test_bin = encoder.fit_transform(y_test.to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Initialize list to store fpr, tpr, and thresholds values\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Loop over each class\n",
    "for i in range(y_test_bin.shape[1]):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_probs[:, i])\n",
    "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = roc_auc_score(y_test_bin[:, i], y_pred_probs[:, i])\n",
    "    aucs.append(roc_auc)\n",
    "    \n",
    "# Compute mean tpr and AUC\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(mean_fpr, mean_tpr, label='Mean ROC (AUC = {:0.2f} $\\pm$ {:0.2f})'.format(mean_auc, std_auc),\n",
    "         color='blue', lw=2, alpha=.8)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Diabetes Prediction')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "#the answer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Import roc_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Plot tpr against fpr\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Diabetes Prediction')\n",
    "plt.show()\n",
    "# Import roc_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Plot tpr against fpr\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Diabetes Prediction')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC\n",
    "\n",
    "The ROC curve you plotted in the last exercise looked promising.\n",
    "\n",
    "Now you will compute the area under the ROC curve, along with the other classification metrics you have used previously.\n",
    "\n",
    "The confusion_matrix and classification_report functions have been preloaded for you, along with the logreg model you previously built, plus X_train, X_test, y_train, y_test. Also, the model's predicted test set labels are stored as y_pred, and probabilities of test set observations belonging to the positive class stored as y_pred_probs.\n",
    "\n",
    "A knn model has also been created and the performance metrics printed in the console, so you can compare the roc_auc_score, confusion_matrix, and classification_report between the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "Import roc_auc_score.\n",
    "\n",
    "Calculate and print the ROC AUC score, passing the test labels and the predicted positive class probabilities.\n",
    "\n",
    "Calculate and print the confusion matrix.\n",
    "\n",
    "Call classification_report()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Import roc_auc_score\\nfrom sklearn.metrics import roc_auc_score\\n\\n# Calculate roc_auc_score\\nprint(roc_auc_score(y_test, y_pred_probs, multi_class='ovr'))\\n\\n# Calculate the confusion matrix\\nprint(confusion_matrix(y_test, y_pred))\\n\\n# Calculate the classification report\\nprint(classification_report(y_test, y_pred))\\n\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Import roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Calculate roc_auc_score\n",
    "print(roc_auc_score(y_test, y_pred_probs, multi_class='ovr'))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Calculate the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "Now you have seen how to perform grid search hyperparameter tuning, you are going to build a lasso regression model with optimal hyperparameters to predict blood glucose levels using the features in the diabetes_df dataset.\n",
    "\n",
    "X_train, X_test, y_train, and y_test have been preloaded for you. A KFold() object has been created and stored for you as kf, along with a lasso regression model as lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "Import GridSearchCV.\n",
    "\n",
    "Set up a parameter grid for \"alpha\", using np.linspace() to create 20 evenly spaced values ranging from 0.00001 to 1.\n",
    "\n",
    "Call GridSearchCV(), passing lasso, the parameter grid, and setting cv equal to kf.\n",
    "\n",
    "Fit the grid search object to the training data to perform a cross-validated grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Import GridSearchCV\\nfrom sklearn.model_selection import GridSearchCV\\n\\n#\\xa0Set up the parameter grid\\nparam_grid = {\"alpha\": np.linspace(0.00001, 1, 20)}\\n\\n# Instantiate lasso_cv\\nlasso_cv = GridSearchCV(lasso, param_grid, cv= kf)\\n\\n# Fit to the training data\\nlasso_cv.fit(X_train,y_train)\\nprint(\"Tuned lasso paramaters: {}\".format(lasso_cv.best_params_))\\nprint(\"Tuned lasso score: {}\".format(lasso_cv.best_score_))\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\"alpha\": np.linspace(0.00001, 1, 20)}\n",
    "\n",
    "# Instantiate lasso_cv\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv= kf)\n",
    "\n",
    "# Fit to the training data\n",
    "lasso_cv.fit(X_train,y_train)\n",
    "print(\"Tuned lasso paramaters: {}\".format(lasso_cv.best_params_))\n",
    "print(\"Tuned lasso score: {}\".format(lasso_cv.best_score_))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning with RandomizedSearchCV\n",
    "As you saw, GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space. In this case, you can use RandomizedSearchCV, which tests a fixed number of hyperparameter settings from specified probability distributions.\n",
    "\n",
    "Training and test sets from diabetes_df have been pre-loaded for you as X_train. X_test, y_train, and y_test, where the target is \"diabetes\". A logistic regression model has been created and stored as logreg, as well as a KFold variable stored as kf.\n",
    "\n",
    "You will define a range of hyperparameters and use RandomizedSearchCV, which has been imported from sklearn.model_selection, to look for optimal hyperparameters from these options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Instructions\n",
    "Create params, adding \"l1\" and \"l2\" as penalty values, setting C to a range of 50 float values between 0.1 and 1.0, and class_weight to either \"balanced\" or a dictionary containing 0:0.8, 1:0.2.\n",
    "Create the Randomized Search CV object, passing the model and the parameters, and setting cv equal to kf.\n",
    "Fit logreg_cv to the training data.\n",
    "Print the model's best parameters and accuracy score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import GridSearchCV\\n\\n#\\xa0Create the parameter space\\nparams = {\"penalty\": [\"l1\", \"l2\"],\\n         \"tol\": np.linspace(0.0001, 1.0, 50),\\n         \"C\": np.linspace(0.1, 1.0, 50),\\n         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]}\\n\\n# Instantiate the RandomizedSearchCV object\\nlogreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\\n\\n# Fit the data to the model\\nlogreg_cv.fit(X_train, y_train)\\n\\n# Print the tuned parameters and score\\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\\nprint(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter space\n",
    "params = {\"penalty\": [\"l1\", \"l2\"],\n",
    "         \"tol\": np.linspace(0.0001, 1.0, 50),\n",
    "         \"C\": np.linspace(0.1, 1.0, 50),\n",
    "         \"class_weight\": [\"balanced\", {0:0.8, 1:0.2}]}\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object\n",
    "logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)\n",
    "\n",
    "# Fit the data to the model\n",
    "logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with RandomizedSearchCV\n",
    "\n",
    "As you saw, GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space. In this case, you can use RandomizedSearchCV, which tests a fixed number of hyperparameter settings from specified probability distributions.\n",
    "\n",
    "Training and test sets from diabetes_df have been pre-loaded for you as X_train. X_test, y_train, and y_test, where the target is \"diabetes\". A logistic regression model has been created and stored as logreg, as well as a KFold variable stored as kf.\n",
    "\n",
    "You will define a range of hyperparameters and use RandomizedSearchCV, which has been imported from sklearn.model_selection, to look for optimal hyperparameters from these options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "Create params, adding \"l1\" and \"l2\" as penalty values, setting C to a range of 50 float values between 0.1 and 1.0, and class_weight to either \"balanced\" or a dictionary containing 0:0.8, 1:0.2.\n",
    "\n",
    "Create the Randomized Search CV object, passing the model and the parameters, and setting cv equal to kf.\n",
    "\n",
    "Fit logreg_cv to the training data.\n",
    "\n",
    "Print the model's best parameters and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Print the tuned parameters and score\\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\\nprint(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Tuned Logistic Regression Best Accuracy Score: {}\".format(logreg_cv.best_score_))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
